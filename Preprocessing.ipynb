{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "17cc9049",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'en_core_web_sm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-9c335543df26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTfidfTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'en_core_web_sm'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer, WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from string import punctuation\n",
    "import collections\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import en_core_web_sm\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import jaccard_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a458ad8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "508865ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "374a95fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = cnxn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "6a490d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = cursor.execute(\"SELECT * from dbo.Users1\").fetchmany(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "a08d88c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame((tuple(t) for t in rows), columns=['id',\n",
    " 'author',\n",
    " 'created_at',\n",
    " 'location',\n",
    " 'description',\n",
    " 'verified',\n",
    " 'followers',\n",
    " 'following',\n",
    " 'favourites_count',\n",
    " 'statuses_count',\n",
    " 'lang',\n",
    " 'tweets',\n",
    " 'following_json',\n",
    " 'followers_json']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "3dfbb84a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>created_at</th>\n",
       "      <th>location</th>\n",
       "      <th>description</th>\n",
       "      <th>verified</th>\n",
       "      <th>followers</th>\n",
       "      <th>following</th>\n",
       "      <th>favourites_count</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>lang</th>\n",
       "      <th>tweets</th>\n",
       "      <th>following_json</th>\n",
       "      <th>followers_json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2284857094</td>\n",
       "      <td>themoonisironic</td>\n",
       "      <td>2014-01-16 01:10:33</td>\n",
       "      <td>‚Äò97 // they/them // ????????</td>\n",
       "      <td>where there is hope, there are trials // art: ...</td>\n",
       "      <td>False</td>\n",
       "      <td>101</td>\n",
       "      <td>516</td>\n",
       "      <td>121166</td>\n",
       "      <td>36377</td>\n",
       "      <td>None</td>\n",
       "      <td>[{\"id\": 1356030533780824069, \"created_at\": \"20...</td>\n",
       "      <td>[{\"id\": 1387079251476815875, \"author\": \"rikuno...</td>\n",
       "      <td>[{\"id\": 1384297303129825284, \"author\": \"bts123...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>834324620</td>\n",
       "      <td>bbelita23</td>\n",
       "      <td>2012-09-19 23:21:48</td>\n",
       "      <td>spooky town, PR</td>\n",
       "      <td>just put it out into the universe</td>\n",
       "      <td>False</td>\n",
       "      <td>670</td>\n",
       "      <td>443</td>\n",
       "      <td>33428</td>\n",
       "      <td>92917</td>\n",
       "      <td>None</td>\n",
       "      <td>[{\"id\": 1356026926134140928, \"created_at\": \"20...</td>\n",
       "      <td>[{\"id\": 2513536418, \"author\": \"Kvn_Astacio\", \"...</td>\n",
       "      <td>[{\"id\": 1289669319526420482, \"author\": \"keviin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>175521394</td>\n",
       "      <td>Stormfocus18</td>\n",
       "      <td>2010-08-06 21:03:08</td>\n",
       "      <td>Puerto Rico_Island</td>\n",
       "      <td>Loving #MewGulf and #KristSingto, amante de la...</td>\n",
       "      <td>False</td>\n",
       "      <td>199</td>\n",
       "      <td>187</td>\n",
       "      <td>6497</td>\n",
       "      <td>38945</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"id\": 1387677517822054400, \"author\": \"tinest...</td>\n",
       "      <td>[{\"id\": 1387677517822054400, \"author\": \"tinest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1685491040</td>\n",
       "      <td>CoraimaINegron</td>\n",
       "      <td>2013-08-20 10:48:59</td>\n",
       "      <td>Puerto Rico</td>\n",
       "      <td>‚ÄúEl karma te lo devolver√° todo, excepto a la m...</td>\n",
       "      <td>False</td>\n",
       "      <td>64</td>\n",
       "      <td>203</td>\n",
       "      <td>1336</td>\n",
       "      <td>1565</td>\n",
       "      <td>None</td>\n",
       "      <td>[{\"id\": 1283388114661244934, \"created_at\": \"20...</td>\n",
       "      <td>[{\"id\": 299932350, \"author\": \"DMcIntyreWWE\", \"...</td>\n",
       "      <td>[{\"id\": 713749428935462916, \"author\": \"DavidRo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id           author          created_at  \\\n",
       "0  2284857094  themoonisironic 2014-01-16 01:10:33   \n",
       "1   834324620        bbelita23 2012-09-19 23:21:48   \n",
       "2   175521394     Stormfocus18 2010-08-06 21:03:08   \n",
       "3  1685491040   CoraimaINegron 2013-08-20 10:48:59   \n",
       "\n",
       "                       location  \\\n",
       "0  ‚Äò97 // they/them // ????????   \n",
       "1              spooky town, PR    \n",
       "2           Puerto Rico_Island    \n",
       "3                   Puerto Rico   \n",
       "\n",
       "                                         description  verified  followers  \\\n",
       "0  where there is hope, there are trials // art: ...     False        101   \n",
       "1                  just put it out into the universe     False        670   \n",
       "2  Loving #MewGulf and #KristSingto, amante de la...     False        199   \n",
       "3  ‚ÄúEl karma te lo devolver√° todo, excepto a la m...     False         64   \n",
       "\n",
       "   following  favourites_count  statuses_count  lang  \\\n",
       "0        516            121166           36377  None   \n",
       "1        443             33428           92917  None   \n",
       "2        187              6497           38945  None   \n",
       "3        203              1336            1565  None   \n",
       "\n",
       "                                              tweets  \\\n",
       "0  [{\"id\": 1356030533780824069, \"created_at\": \"20...   \n",
       "1  [{\"id\": 1356026926134140928, \"created_at\": \"20...   \n",
       "2                                                 []   \n",
       "3  [{\"id\": 1283388114661244934, \"created_at\": \"20...   \n",
       "\n",
       "                                      following_json  \\\n",
       "0  [{\"id\": 1387079251476815875, \"author\": \"rikuno...   \n",
       "1  [{\"id\": 2513536418, \"author\": \"Kvn_Astacio\", \"...   \n",
       "2  [{\"id\": 1387677517822054400, \"author\": \"tinest...   \n",
       "3  [{\"id\": 299932350, \"author\": \"DMcIntyreWWE\", \"...   \n",
       "\n",
       "                                      followers_json  \n",
       "0  [{\"id\": 1384297303129825284, \"author\": \"bts123...  \n",
       "1  [{\"id\": 1289669319526420482, \"author\": \"keviin...  \n",
       "2  [{\"id\": 1387677517822054400, \"author\": \"tinest...  \n",
       "3  [{\"id\": 713749428935462916, \"author\": \"DavidRo...  "
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "2a82af12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>geo</th>\n",
       "      <th>user</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1283388114661244934</td>\n",
       "      <td>2020-07-15T13:09:05</td>\n",
       "      <td>Nada m√°s cierto! ü§¶üèª‚Äç‚ôÄÔ∏èü§¶üèª‚Äç‚ôÄÔ∏èü§¶üèª‚Äç‚ôÄÔ∏è https://t.co/...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>CoraimaINegron</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1282263167511470082</td>\n",
       "      <td>2020-07-12T10:38:57</td>\n",
       "      <td>Llamen a mi jefe diganle que la lluvia no me d...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>CoraimaINegron</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1280867816544047107</td>\n",
       "      <td>2020-07-08T14:14:19</td>\n",
       "      <td>RT @SaraJarque: https://t.co/olOHb7CC9R</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>CoraimaINegron</td>\n",
       "      <td>403</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1280867775209234433</td>\n",
       "      <td>2020-07-08T14:14:09</td>\n",
       "      <td>RT @SaraJarque: 1, 2, 3 Pescao! Pescao! https:...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>CoraimaINegron</td>\n",
       "      <td>205</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1280439524485718022</td>\n",
       "      <td>2020-07-07T09:52:26</td>\n",
       "      <td>@kefvelazquez As√≠ est√° bien o todav√≠a no estas...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>CoraimaINegron</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id           created_at  \\\n",
       "0  1283388114661244934  2020-07-15T13:09:05   \n",
       "1  1282263167511470082  2020-07-12T10:38:57   \n",
       "2  1280867816544047107  2020-07-08T14:14:19   \n",
       "3  1280867775209234433  2020-07-08T14:14:09   \n",
       "4  1280439524485718022  2020-07-07T09:52:26   \n",
       "\n",
       "                                                text coordinates   geo  \\\n",
       "0  Nada m√°s cierto! ü§¶üèª‚Äç‚ôÄÔ∏èü§¶üèª‚Äç‚ôÄÔ∏èü§¶üèª‚Äç‚ôÄÔ∏è https://t.co/...        None  None   \n",
       "1  Llamen a mi jefe diganle que la lluvia no me d...        None  None   \n",
       "2            RT @SaraJarque: https://t.co/olOHb7CC9R        None  None   \n",
       "3  RT @SaraJarque: 1, 2, 3 Pescao! Pescao! https:...        None  None   \n",
       "4  @kefvelazquez As√≠ est√° bien o todav√≠a no estas...        None  None   \n",
       "\n",
       "             user  retweet_count  favorite_count  \n",
       "0  CoraimaINegron              0               0  \n",
       "1  CoraimaINegron              0               0  \n",
       "2  CoraimaINegron            403               0  \n",
       "3  CoraimaINegron            205               0  \n",
       "4  CoraimaINegron              0               1  "
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "for x in df.tweets: \n",
    "    y = json.loads(x)\n",
    "    \n",
    "tweets = pd.DataFrame(y) \n",
    "tweets.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "54be1b09",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character '‚Äú' (U+201C) (<ipython-input-98-b407aec837de>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-98-b407aec837de>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    df[text_field] = df[text_field].apply(lambda elem: re.sub(r\"(@[A-Za-z0‚Äì9]+)|([‚Å∞-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", ‚Äú‚Äù, elem))\u001b[0m\n\u001b[0m                                                                                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character '‚Äú' (U+201C)\n"
     ]
    }
   ],
   "source": [
    "# remove the hashtags, mentions and unwanted characters.\n",
    "def clean_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    df[text_field] = df[text_field].apply(lambda elem: re.sub(r\"(@[A-Za-z0‚Äì9]+)|([‚Å∞-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", ‚Äú‚Äù, elem)) \n",
    "    return df\n",
    "tweets_bowl = clean_text(tweets, ‚Äòtweets‚Äô)\n",
    "tweets_bowl.head()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "61c9cdad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>geo</th>\n",
       "      <th>user</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1356040835469635584</td>\n",
       "      <td>2021-02-01T00:45:03</td>\n",
       "      <td>RT @pipitostgo: Acho si fuera Yulin la que est...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Gatacielo</td>\n",
       "      <td>129</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1355630751933493254</td>\n",
       "      <td>2021-01-30T21:35:32</td>\n",
       "      <td>RT @OComejen: -ROMPIENDO-\\nAparece el fantasma...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Gatacielo</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1355630670706577408</td>\n",
       "      <td>2021-01-30T21:35:12</td>\n",
       "      <td>RT @josecrates: El panorama mundial es de no a...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Gatacielo</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1355626611085893642</td>\n",
       "      <td>2021-01-30T21:19:05</td>\n",
       "      <td>RT @LISI_13: Y el gran problema son los confor...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Gatacielo</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1355624457117827072</td>\n",
       "      <td>2021-01-30T21:10:31</td>\n",
       "      <td>RT @elsbtmar: All√° los que les encanta que les...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Gatacielo</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id           created_at  \\\n",
       "0  1356040835469635584  2021-02-01T00:45:03   \n",
       "1  1355630751933493254  2021-01-30T21:35:32   \n",
       "2  1355630670706577408  2021-01-30T21:35:12   \n",
       "3  1355626611085893642  2021-01-30T21:19:05   \n",
       "4  1355624457117827072  2021-01-30T21:10:31   \n",
       "\n",
       "                                                text coordinates   geo  \\\n",
       "0  RT @pipitostgo: Acho si fuera Yulin la que est...        None  None   \n",
       "1  RT @OComejen: -ROMPIENDO-\\nAparece el fantasma...        None  None   \n",
       "2  RT @josecrates: El panorama mundial es de no a...        None  None   \n",
       "3  RT @LISI_13: Y el gran problema son los confor...        None  None   \n",
       "4  RT @elsbtmar: All√° los que les encanta que les...        None  None   \n",
       "\n",
       "        user  retweet_count  favorite_count  \n",
       "0  Gatacielo            129               0  \n",
       "1  Gatacielo              5               0  \n",
       "2  Gatacielo              5               0  \n",
       "3  Gatacielo              3               0  \n",
       "4  Gatacielo             30               0  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "b70e074d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Preprocessor √≠ is üëç'"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import preprocessor as p\n",
    "p.set_options(p.OPT.URL,p.OPT.MENTION,p.OPT.HASHTAG,p.OPT.RESERVED,p.OPT.SMILEY,p.OPT.NUMBER)\n",
    "p.clean('Preprocessor √≠ is #awesome üëç https://github.com/s/preprocessor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "b4d49195",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuits = []\n",
    "for tweet in tweets.text:\n",
    "    tuits.append(p.clean(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "5cfcbb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "\n",
    "\n",
    "'''Natural Language Processing libraries'''\n",
    "import nltk \n",
    "import gensim\n",
    "import regex as re\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "import re, string, unicodedata\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from gensim.models import FastText\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "\n",
    "class Document:\n",
    "    \"\"\" Retrieve the narratives from the DataFrame and respectively\n",
    "        store and pre-process it. \n",
    "        \n",
    "        :param df: DataFrame including the reports and the predictor variable. \n",
    "        \n",
    "        \n",
    "        :ivar data: Stores the DataFrame.\n",
    "        :ivar text: Stores the narratives as string.\n",
    "        :ivar corpus: Stores the pre-processed text.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.data = df\n",
    "        self.text = df[\"text\"].astype(str)\n",
    "        self.textPreProcessing()\n",
    "        \n",
    "        \n",
    "    def remove_non_ascii(self, words):\n",
    "        \"\"\"Remove non-ASCII characters from list of tokenized words\n",
    "        \n",
    "        :param words:  List of words to be transformed when removing non_ascii characters.\n",
    "        \n",
    "        :return new_words: List of words after the transformation of removed non_ascii characters.\n",
    "        \n",
    "        \"\"\"\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            new_words.append(new_word)\n",
    "        return new_words\n",
    "\n",
    "\n",
    "    def remove_punctuation(self, words):\n",
    "        \"\"\"Remove punctuation from list of tokenized words\n",
    "        \n",
    "        :param words:  List of words that will get remove their punctuations, if any. \n",
    "        \n",
    "        :return new_words: List of transformed words.\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        new_words = []\n",
    "        for word in words:\n",
    "            new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "            if new_word != '':\n",
    "                new_words.append(new_word)\n",
    "        return new_words\n",
    "\n",
    "\n",
    "    def stem_words(self, words):\n",
    "        \"\"\"Stem words in list of tokenized words\n",
    "        \n",
    "        :param words:  List of words to be processed. \n",
    "        \n",
    "        :return new_words: List of the received words respective stems.\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        stemmer = LancasterStemmer()\n",
    "        stems = []\n",
    "        for word in words:\n",
    "            stem = stemmer.stem(word)\n",
    "            stems.append(stem)\n",
    "        return stems\n",
    "    \n",
    "    def lemmatize_verbs(self, words):\n",
    "        \"\"\"Lemmatize verbs in list of tokenized words\n",
    "        \n",
    "        :param words:  List of words to be processed. \n",
    "        \n",
    "        :return new_words: List of the received words respective lemmas.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmas = []\n",
    "        for word in words:\n",
    "            lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "            lemmas.append(lemma)\n",
    "        return lemmas\n",
    "    \n",
    "    \n",
    "    \n",
    "    def remove_stopwords(self, words):\n",
    "        \"\"\"Remove common words that have no meaning or importance in the sentence.\n",
    "\n",
    "        :param words:  List of words to be processed and get stop words removed.. \n",
    "\n",
    "        :return new_words: List of words with the stop words already removed.\"\"\"\n",
    "            \n",
    "        \n",
    "        stop_words = set(stopwords.words('spanish')) \n",
    "        \n",
    "        for word in stop_words:\n",
    "            if word in words:\n",
    "                words.remove(word)\n",
    "                \n",
    "        return words\n",
    "\n",
    "\n",
    "    \n",
    "    def normalize(self, words):\n",
    "        words = self.remove_non_ascii(words)\n",
    "        words = self.remove_stopwords(words)\n",
    "        words = self.remove_punctuation(words)\n",
    "        words = self.lemmatize_verbs(words)\n",
    "        return words\n",
    "    \n",
    "    \n",
    "    def textPreProcessing(self):\n",
    "        \"\"\"Pre-process the text, normalize and clean it.\n",
    "        The function stores the cleaned text in the self.data\n",
    "        attribute. \"\"\"\n",
    "\n",
    "        clean_text = []\n",
    "\n",
    "        for narrative in self.text:\n",
    "            \n",
    "            sentence = word_tokenize(narrative)\n",
    "            sentence = self.normalize(sentence)\n",
    "                \n",
    "                \n",
    "            clean_text.append(sentence)\n",
    "            \n",
    "            \n",
    "                    \n",
    "        print(len(self.text), len(clean_text))\n",
    "        self.data[\"clean_text\"] = clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "bfee8576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/bryanortiz/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/bryanortiz/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/bryanortiz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "texto = pd.DataFrame(tuits, columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "bc278123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118 118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/bryanortiz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/bryanortiz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "documento = Document(texto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "cdd7a29e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                    [Nada, mas, cierto]\n",
       "1      [Llamen, jefe, diganle, lluvia, deja, ir, trab...\n",
       "2                                                     []\n",
       "3                                       [Pescao, Pescao]\n",
       "4      [Asi, bien, todavia, tan, sorprendido, El, gol...\n",
       "                             ...                        \n",
       "113    [El, ajusta, magnitud, limite, producir, tsunami]\n",
       "114    [Van, seguir, ocurriendo, eventos, fuertes, Ha...\n",
       "115                             [Casa, colapsada, Yauco]\n",
       "116    [Las, replicas, podrian, ser, magnitud, mayor,...\n",
       "117    [PRELIMINAR, 0106, 5803, No, Aviso, Advertenci...\n",
       "Name: clean_text, Length: 118, dtype: object"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documento.data[\"clean_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "227036a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(documento.data[\"clean_text\"], min_count=2, workers=20, window=2,  alpha=0.02, hs=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "db1351e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dict = dict({})\n",
    "for idx, key in enumerate(model.wv.key_to_index):\n",
    "    my_dict[key] = model.wv[key]\n",
    "    # Or my_dict[key] = model.wv.get_vector(key)\n",
    "    # Or my_dict[key] = model.wv.word_vec(key, use_norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "ee868e4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>vectorized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nada m√°s cierto! ü§¶üèª‚Äç‚ôÄÔ∏èü§¶üèª‚Äç‚ôÄÔ∏èü§¶üèª‚Äç‚ôÄÔ∏è</td>\n",
       "      <td>[Nada, mas, cierto]</td>\n",
       "      <td>[[0.009654882, 0.007363939, 0.0012740177, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Llamen a mi jefe diganle que la lluvia no me d...</td>\n",
       "      <td>[Llamen, jefe, diganle, lluvia, deja, ir, trab...</td>\n",
       "      <td>[[0.009929657, 0.0024249798, 0.0051007434, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>:</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>: , , Pescao! Pescao!</td>\n",
       "      <td>[Pescao, Pescao]</td>\n",
       "      <td>[[0.0023572692, 0.00421269, 0.0019974997, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>As√≠ est√° bien o todav√≠a no estas tan sorprendi...</td>\n",
       "      <td>[Asi, bien, todavia, tan, sorprendido, El, gol...</td>\n",
       "      <td>[[0.004190841, 0.002390885, -0.0003632684, -0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                   Nada m√°s cierto! ü§¶üèª‚Äç‚ôÄÔ∏èü§¶üèª‚Äç‚ôÄÔ∏èü§¶üèª‚Äç‚ôÄÔ∏è   \n",
       "1  Llamen a mi jefe diganle que la lluvia no me d...   \n",
       "2                                                  :   \n",
       "3                              : , , Pescao! Pescao!   \n",
       "4  As√≠ est√° bien o todav√≠a no estas tan sorprendi...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0                                [Nada, mas, cierto]   \n",
       "1  [Llamen, jefe, diganle, lluvia, deja, ir, trab...   \n",
       "2                                                 []   \n",
       "3                                   [Pescao, Pescao]   \n",
       "4  [Asi, bien, todavia, tan, sorprendido, El, gol...   \n",
       "\n",
       "                                     vectorized_text  \n",
       "0  [[0.009654882, 0.007363939, 0.0012740177, -0.0...  \n",
       "1  [[0.009929657, 0.0024249798, 0.0051007434, -0....  \n",
       "2                                                 []  \n",
       "3  [[0.0023572692, 0.00421269, 0.0019974997, 0.00...  \n",
       "4  [[0.004190841, 0.002390885, -0.0003632684, -0....  "
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectores = []\n",
    "for a in documento.data.clean_text:\n",
    "    t  = []\n",
    "    for word in a:\n",
    "        try:\n",
    "            t.append(my_dict[word])\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    vectores.append(t)\n",
    "        \n",
    "documento.data[\"vectorized_text\"] = vectores\n",
    "documento.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "ca021fff",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Van'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-248-b51feeb716f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Van\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'Van'"
     ]
    }
   ],
   "source": [
    "my_dict[\"Van\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "f70426e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'de': 0,\n",
       " 'No': 1,\n",
       " 'a': 2,\n",
       " 'Aviso': 3,\n",
       " 'Bayamon': 4,\n",
       " 'Nuevo': 5,\n",
       " 'Barrio': 6,\n",
       " 'El': 7,\n",
       " 'la': 8,\n",
       " 'magnitud': 9,\n",
       " 'Ml': 10,\n",
       " 'Sentido': 11,\n",
       " 'Km': 12,\n",
       " 'Prof': 13,\n",
       " 'Mag': 14,\n",
       " 'PRELIMINAR': 15,\n",
       " 'Max': 16,\n",
       " 'Puerto': 17,\n",
       " 'Rico': 18,\n",
       " 'Int': 19,\n",
       " 'temblor': 20,\n",
       " 'PR': 21,\n",
       " 'enero2020': 22,\n",
       " 'PM': 23,\n",
       " 'que': 24,\n",
       " 'mas': 25,\n",
       " 'tsunami': 26,\n",
       " 'be': 27,\n",
       " 'sabado': 28,\n",
       " 'luz': 29,\n",
       " 'servicio': 30,\n",
       " 'hoy': 31,\n",
       " 'La': 32,\n",
       " 'Gracias': 33,\n",
       " 'los': 34,\n",
       " 'Wanda': 35,\n",
       " 'Hoy': 36,\n",
       " 'mayor': 37,\n",
       " 'mejor': 38,\n",
       " 'ser': 39,\n",
       " 'el': 40,\n",
       " 'vez': 41,\n",
       " 'Que': 42,\n",
       " 'Mayaguez': 43,\n",
       " 'gente': 44,\n",
       " 'fuerte': 45,\n",
       " 'ver': 46,\n",
       " 'si': 47,\n",
       " 'l': 48,\n",
       " 'Mas': 49,\n",
       " 'cada': 50,\n",
       " 'ir': 51,\n",
       " 'chicharra': 52,\n",
       " 'con': 53,\n",
       " 'preliminar': 54,\n",
       " 'Islas': 55,\n",
       " 'Advertencia': 56,\n",
       " 'Vigilancia': 57,\n",
       " 'no': 58,\n",
       " 'Los': 59,\n",
       " 'ACTUALIZADA': 60,\n",
       " 'temblores': 61,\n",
       " 'pandemia': 62,\n",
       " 'Lo': 63,\n",
       " 'Enero': 64,\n",
       " 'tan': 65,\n",
       " 'espero': 66,\n",
       " 'isla': 67,\n",
       " 'Hola': 68,\n",
       " 'cierto': 69,\n",
       " 'Pescao': 70,\n",
       " 'bien': 71,\n",
       " 'vi': 72,\n",
       " 'NO': 73,\n",
       " 'entrada': 74,\n",
       " 've': 75,\n",
       " 'pm': 76,\n",
       " 'pico': 77,\n",
       " 'TEMBLOR': 78,\n",
       " 'IV': 79,\n",
       " 'solamente': 80,\n",
       " 'malo': 81,\n",
       " 'risa': 82,\n",
       " 'muriendo': 83,\n",
       " 'dia': 84,\n",
       " 'Molinelli': 85,\n",
       " 'va': 86,\n",
       " 'quedense': 87,\n",
       " 'seguridad': 88,\n",
       " 'por': 89,\n",
       " 'mia': 90,\n",
       " 'sus': 91,\n",
       " 'Jajajajaja': 92,\n",
       " 'Un': 93,\n",
       " 'pa': 94,\n",
       " 'Yauco': 95,\n",
       " 'PTWC': 96,\n",
       " 'dias': 97,\n",
       " '0236': 98,\n",
       " 'V': 99,\n",
       " 'Ponce': 100,\n",
       " 'Adverten': 101,\n",
       " 'Temblor': 102,\n",
       " 'danos': 103,\n",
       " 'Es': 104,\n",
       " 'duermo': 105,\n",
       " 'I': 106,\n",
       " 'don': 107,\n",
       " 'slip': 108,\n",
       " 'tudey': 109,\n",
       " 'Virgenes': 110,\n",
       " 'M': 111,\n",
       " 'Y': 112,\n",
       " 'base': 113,\n",
       " 'estadounidenses': 114,\n",
       " 'casa': 115,\n",
       " 'consiencia': 116,\n",
       " 'viene': 117,\n",
       " 'Tomen': 118,\n",
       " 'A': 119,\n",
       " 'vamos': 120,\n",
       " 'dicen': 121,\n",
       " 'nunca': 122,\n",
       " 'ayer': 123,\n",
       " 'Jose': 124,\n",
       " 'manana': 125,\n",
       " 'Mi': 126,\n",
       " 'En': 127,\n",
       " 'principal': 128,\n",
       " 'un': 129,\n",
       " 'Dos': 130,\n",
       " 'las': 131,\n",
       " 'pediatra': 132,\n",
       " 'San': 133,\n",
       " 'luego': 134,\n",
       " 'empleados': 135,\n",
       " 'supermercados': 136,\n",
       " 'farmacias': 137,\n",
       " 'seguir': 138,\n",
       " 'Hagamos': 139,\n",
       " 'gracias': 140,\n",
       " 'noche': 141,\n",
       " 'trafico': 142,\n",
       " 'Ayuda': 143,\n",
       " 'profesional': 144,\n",
       " 'salud': 145,\n",
       " 'seria': 146}"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fc50f1f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-d6ba9348fabc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext_field\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext_field\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"(@[A-Za-z0‚Äì9]+)|([‚Å∞-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtweets_bowl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtweets_bowl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-118-d6ba9348fabc>\u001b[0m in \u001b[0;36mclean_text\u001b[0;34m(df, text_field)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# remove the hashtags, mentions and unwanted characters.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_field\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext_field\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext_field\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext_field\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext_field\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"(@[A-Za-z0‚Äì9]+)|([‚Å∞-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "# remove the hashtags, mentions and unwanted characters.\n",
    "def clean_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    df[text_field] = df[text_field].apply(lambda elem: re.sub(r\"(@[A-Za-z0‚Äì9]+)|([‚Å∞-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\",   elem)) \n",
    "    return df\n",
    "tweets_bowl = clean_text(tuits, 'text')\n",
    "tweets_bowl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc03150",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
